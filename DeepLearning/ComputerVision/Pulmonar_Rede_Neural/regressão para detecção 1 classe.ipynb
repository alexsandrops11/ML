{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from pyimagesearch import config\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import os\n",
    "# define the base path to the input dataset and then use it to derive\n",
    "# the path to the images directory and annotation CSV file\n",
    "BASE_PATH = \"dataset\"\n",
    "IMAGES_PATH = os.path.sep.join([BASE_PATH, \"images\"])\n",
    "ANNOTS_PATH = os.path.sep.join([BASE_PATH, \"airplanes.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to the base output directory\n",
    "BASE_OUTPUT = \"output\"\n",
    "# define the path to the output serialized model, model training plot,\n",
    "# and testing image filenames\n",
    "MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"detector.h5\"])\n",
    "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])\n",
    "TEST_FILENAMES = os.path.sep.join([BASE_OUTPUT, \"test_images.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 1e-4\n",
    "NUM_EPOCHS = 25\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the contents of the CSV annotations file\n",
    "print(\"[INFO] loading dataset...\")\n",
    "rows = open(config.ANNOTS_PATH).read().strip().split(\"\\n\")\n",
    "# initialize the list of data (images), our target output predictions\n",
    "# (bounding box coordinates), along with the filenames of the\n",
    "# individual images\n",
    "data = []\n",
    "targets = []\n",
    "filenames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the rows\n",
    "for row in rows:\n",
    "    # break the row into the filename and bounding box coordinates\n",
    "    row = row.split(\",\")\n",
    "    (filename, startX, startY, endX, endY) = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive the path to the input image, load the image (in OpenCV\n",
    "# format), and grab its dimensions\n",
    "imagePath = os.path.sep.join([config.IMAGES_PATH, filename])\n",
    "image = cv2.imread(imagePath)\n",
    "(h, w) = image.shape[:2]\n",
    "# scale the bounding box coordinates relative to the spatial\n",
    "# dimensions of the input image\n",
    "startX = float(startX) / w\n",
    "startY = float(startY) / h\n",
    "endX = float(endX) / w\n",
    "endY = float(endY) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image and preprocess it\n",
    "image = load_img(imagePath, target_size=(224, 224))\n",
    "image = img_to_array(image)\n",
    "# update our list of data, targets, and filenames\n",
    "data.append(image)\n",
    "targets.append((startX, startY, endX, endY))\n",
    "filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data and targets to NumPy arrays, scaling the input\n",
    "# pixel intensities from the range [0, 255] to [0, 1]\n",
    "data = np.array(data, dtype=\"float32\") / 255.0\n",
    "targets = np.array(targets, dtype=\"float32\")\n",
    "# partition the data into training and testing splits using 90% of\n",
    "# the data for training and the remaining 10% for testing\n",
    "split = train_test_split(data, targets, filenames, test_size=0.10,\n",
    "random_state=42)\n",
    "# unpack the data split\n",
    "(trainImages, testImages) = split[:2]\n",
    "(trainTargets, testTargets) = split[2:4]\n",
    "(trainFilenames, testFilenames) = split[4:]\n",
    "# write the testing filenames to disk so that we can use then\n",
    "# when evaluating/testing our bounding box regressor\n",
    "print(\"[INFO] saving testing filenames...\")\n",
    "f = open(config.TEST_FILENAMES, \"w\")\n",
    "f.write(\"\\n\".join(testFilenames))\n",
    "f.close("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the VGG16 network, ensuring the head FC layers are left off\n",
    "vgg = VGG16(weights=\"imagenet\", include_top=False,\n",
    "input_tensor=Input(shape=(224, 224, 3)))\n",
    "# freeze all VGG layers so they will *not* be updated during the\n",
    "# training process\n",
    "vgg.trainable = False\n",
    "# flatten the max-pooling output of VGG\n",
    "flatten = vgg.output\n",
    "flatten = Flatten()(flatten)\n",
    "# construct a fully-connected layer header to output the predicted\n",
    "# bounding box coordinates\n",
    "bboxHead = Dense(128, activation=\"relu\")(flatten)\n",
    "bboxHead = Dense(64, activation=\"relu\")(bboxHead)\n",
    "bboxHead = Dense(32, activation=\"relu\")(bboxHead)\n",
    "bboxHead = Dense(4, activation=\"sigmoid\")(bboxHead)\n",
    "# construct the model we will fine-tune for bounding box regression\n",
    "model = Model(inputs=vgg.input, outputs=bboxHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the optimizer, compile the model, and show the model\n",
    "# summary\n",
    "opt = Adam(lr=config.INIT_LR)\n",
    "model.compile(loss=\"mse\", optimizer=opt)\n",
    "print(model.summary())\n",
    "# train the network for bounding box regression\n",
    "print(\"[INFO] training bounding box regressor...\")\n",
    "H = model.fit(\n",
    "trainImages, trainTargets,\n",
    "validation_data=(testImages, testTargets),\n",
    "batch_size=config.BATCH_SIZE,\n",
    "epochs=config.NUM_EPOCHS,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-belgium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the model to disk\n",
    "print(\"[INFO] saving object detector model...\")\n",
    "model.save(config.MODEL_PATH, save_format=\"h5\")\n",
    "# plot the model training history\n",
    "N = config.NUM_EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Bounding Box Regression Loss on Training Set\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(config.PLOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from pyimagesearch import config\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import mimetypes\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--input\", required=True,\n",
    "help=\"path to input image/text file of image filenames\")\n",
    "args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the input file type, but assume that we're working with\n",
    "# single input image\n",
    "filetype = mimetypes.guess_type(args[\"input\"])[0]\n",
    "imagePaths = [args[\"input\"]]\n",
    "# if the file type is a text file, then we need to process *multiple*\n",
    "# images\n",
    "if \"text/plain\" == filetype:\n",
    "    # load the filenames in our testing file and initialize our list\n",
    "    # of image paths\n",
    "    filenames = open(args[\"input\"]).read().strip().split(\"\\n\")\n",
    "    imagePaths = []\n",
    "    # loop over the filenames\n",
    "    for f in filenames:\n",
    "        # construct the full path to the image filename and then\n",
    "        # update our image paths list\n",
    "        p = os.path.sep.join([config.IMAGES_PATH, f])\n",
    "        imagePaths.append(p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our trained bounding box regressor from disk\n",
    "print(\"[INFO] loading object detector...\")\n",
    "model = load_model(config.MODEL_PATH)\n",
    "# loop over the images that we'll be testing using our bounding box\n",
    "# regression model\n",
    "for imagePath in imagePaths:\n",
    "    # load the input image (in Keras format) from disk and preprocess\n",
    "    # it, scaling the pixel intensities to the range [0, 1]\n",
    "    image = load_img(imagePath, target_size=(224, 224))\n",
    "    image = img_to_array(image) / 255.0\n",
    "    image = np.expand_dims(image, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bounding box predictions on the input image\n",
    "preds = model.predict(image)[0]\n",
    "(startX, startY, endX, endY) = preds\n",
    "# load the input image (in OpenCV format), resize it such that it\n",
    "# fits on our screen, and grab its dimensions\n",
    "image = cv2.imread(imagePath)\n",
    "image = imutils.resize(image, width=600)\n",
    "(h, w) = image.shape[:2]\n",
    "# scale the predicted bounding box coordinates based on the image\n",
    "# dimensions\n",
    "startX = int(startX * w)\n",
    "startY = int(startY * h)\n",
    "endX = int(endX * w)\n",
    "endY = int(endY * h)\n",
    "# draw the predicted bounding box on the image\n",
    "cv2.rectangle(image, (startX, startY), (endX, endY),\n",
    "    (0, 255, 0), 2)\n",
    "# show the output image\n",
    "cv2.imshow(\"Output\", image)\n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
